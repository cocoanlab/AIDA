{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c4f3c-1c5d-412e-b2e0-4a253c475c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab88ee-d173-460d-97cc-3485eabccfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_dir = \"/home/jaejoong/cocoanlab02\"\n",
    "result_dir = os.path.join(nas_dir, \"projects/AIDA/results\")\n",
    "config_dir = os.path.join(nas_dir, \"projects/AIDA/config\")\n",
    "prompt_dir = os.path.join(nas_dir, \"projects/AIDA/prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae54eb5-1299-462b-9156-544806a7f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(os.path.join(result_dir, \"AIDA_all_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf068c43-ea4b-4bd8-8a10-55047f91e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_dir, \"HuggingFace_token.txt\"), 'r') as file:\n",
    "    HF_token = file.read()\n",
    "login(token=HF_token)\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e784e8-c738-4e93-92f9-ee6e07137bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(prompt_dir, \"AIDA_Prompts.txt\"), 'r') as file:\n",
    "    prompt = json.load(file)\n",
    "    \n",
    "print(\"\\n\\n-----\\n\\n\".join([f\"## {qtype}\\n\\n**Prompt**\\n{qs[\"Prompt\"]}\\n\\n**Question**\\n{\"\\n\".join([\"- \" + q for q in qs[\"Question\"]])}\" for qtype, qs in prompt.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f392b0-444e-4e75-a41c-c8cf089648b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "changeprompt = f\"Reword the following LLM prompt. The last sentence should be \\\"Return only the score.\\\" Return only the modified LLM prompt.\\n\\n[LLM prompt]\\n{prompt[\"PHQ8\"][\"Prompt\"]}\\n\"\n",
    "print(changeprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52752f-2eab-40b0-97a0-ceb1cbfa34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_list = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": changeprompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    msg_list,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=[\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ],\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=1e-4,\n",
    "        top_p=0,\n",
    "    )\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "decoded_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "for qtype, qs in prompt.items():\n",
    "    prompt[qtype][\"Prompt\"] = decoded_response\n",
    "\n",
    "with open(os.path.join(prompt_dir, \"AIDA_Prompts_changed.txt\"), 'w') as file:\n",
    "    json.dump(prompt, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\\n-----\\n\\n\".join([f\"## {qtype}\\n\\n**Prompt**\\n{qs[\"Prompt\"]}\\n\\n**Question**\\n{\"\\n\".join([\"- \" + q for q in qs[\"Question\"]])}\" for qtype, qs in prompt.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca6517-df04-467d-a52d-d67530f33c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resp = []\n",
    "time_start = time.time()\n",
    "print(f\"\\rDialogue {0} / {len(all_df)}, {list(prompt.keys())[0]} {0} / {len(list(prompt.values())[0][\"Question\"])}, elapsed time: {time.strftime(\"%H:%M:%S\", time.gmtime(0))}       \", end=\"\", flush=True)\n",
    "\n",
    "for idx, row in all_df.iterrows():\n",
    "\n",
    "    all_resp.append({})\n",
    "\n",
    "    for qtype, qs in prompt.items():\n",
    "        \n",
    "        all_resp[idx][qtype] = []\n",
    "\n",
    "        for qidx, q in enumerate(qs[\"Question\"]):\n",
    "        \n",
    "            msg_list = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": qs[\"Prompt\"].replace(\"[INSERT_INTERVIEW]\", row[\"Dialogue\"]).replace(\"[INSERT_QUESTION]\", q)},\n",
    "            ]\n",
    "            \n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                msg_list,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            torch.manual_seed(42)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=1,\n",
    "                    eos_token_id=[\n",
    "                        tokenizer.eos_token_id,\n",
    "                        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "                    ],\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=True,\n",
    "                    temperature=1e-4,\n",
    "                    top_p=0,\n",
    "                )\n",
    "            \n",
    "            response = outputs[0][input_ids.shape[-1]:]\n",
    "            decoded_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "            if not decoded_response.isdigit():\n",
    "                decoded_response = \"0\"\n",
    "            all_resp[idx][qtype].append(decoded_response)\n",
    "        \n",
    "            print(f\"\\rDialogue {idx+1} / {len(all_df)}, {qtype} {qidx+1} / {len(qs[\"Question\"])}, elapsed time: {time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - time_start))}       \", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce423931-764f-4b19-b543-2bf7218f3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resp_df = pd.DataFrame([[float(r3) for r2 in r1.values() for r3 in r2] for r1 in all_resp])\n",
    "numq = {k: len(v) for k, v in all_resp[0].items()}\n",
    "all_resp_df.columns = [f\"Q{i+1}\" for i in range(numq[\"PHQ8\"] + numq[\"Ling\"] + numq[\"CDS\"])] + [f\"NQ{i+1}\" for i in range(numq[\"Neu\"])] + [f\"DQ{i+1}\" for i in range(numq[\"DepSev\"])]\n",
    "display(all_resp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c660a83-b8af-4e2b-afa5-32db36c88c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resp_df.to_csv(os.path.join(result_dir, \"AIDA_all_resp_changeprompt_df.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533a344-dd92-4069-a4bf-8ac68a22edec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
